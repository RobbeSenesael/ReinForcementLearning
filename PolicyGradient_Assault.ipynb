{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLearning(scores, filename, x=None, window=5):   \n",
    "    N = len(scores)\n",
    "    running_avg = np.empty(N)\n",
    "    for t in range(N):\n",
    "        running_avg[t] = np.mean(scores[max(0, t-window):(t+1)])\n",
    "    if x is None:\n",
    "        x = [i for i in range(N)]\n",
    "    plt.ylabel('Score')       \n",
    "    plt.xlabel('Game')                     \n",
    "    plt.plot(x, running_avg)\n",
    "    plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientNetwork(keras.Model):\n",
    "    def __init__(self, action_shape):\n",
    "        super(PolicyGradientNetwork, self).__init__()\n",
    "        self.action_shape = action_shape\n",
    "        self.fc1 = Dense(256, activation='relu') \n",
    "        self.fc2 = Dense(256, activation='relu') \n",
    "        self.outp = Dense(action_shape, activation='softmax') \n",
    "        \n",
    "    \n",
    "    def call(self, state): \n",
    "        value = self.fc1(state)\n",
    "        value = self.fc2(value)\n",
    "        outp = self.outp(value)      \n",
    "        \n",
    "        return outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, alpha=0.003, gamma=0.99, n_actions=7, fc1_dims=256, fc2_dims=256):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = []\n",
    "        self.policy = PolicyGradientNetwork(action_shape=n_actions)\n",
    "        self.policy.compile(optimizer=Adam(learning_rate=self.alpha))\n",
    "     \n",
    "    def choose_action(self, observation):\n",
    "        \n",
    "        state = tf.convert_to_tensor(observation)\n",
    "        probs = self.policy(state)\n",
    "        action_probs = tfp.distributions.Categorical(probs=probs)\n",
    "        \n",
    "        action = action_probs.sample()\n",
    "        \n",
    "        return action.numpy()[0].astype(int)[0]\n",
    "\n",
    "    def store_transition(self, observation, action, reward):\n",
    "        self.state_memory.append(observation)\n",
    "        self.action_memory.append(action)\n",
    "        self.reward_memory.append(reward)\n",
    "    \n",
    "    def train(self):\n",
    "        actions = tf.convert_to_tensor(self.action_memory, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(self.reward_memory)\n",
    "\n",
    "        G = np.zeros_like(rewards)\n",
    "        for t in range(len(rewards)):\n",
    "            G_sum = 0\n",
    "            discount = 1\n",
    "            for k in range(t, len(rewards)):\n",
    "                G_sum += rewards[k]*discount\n",
    "                discount *= self.gamma\n",
    "            G[t] = G_sum\n",
    "       \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = 0\n",
    "            for idx, (g, state) in enumerate(zip(G, self.state_memory)):\n",
    "                state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "                state = tf.expand_dims(state, 0)\n",
    "                probs = self.policy(state)\n",
    "                action_probs = tfp.distributions.Categorical(probs=probs)\n",
    "                log_prob = action_probs.log_prob(actions[idx])\n",
    "                loss += -g * tf.squeeze(log_prob)\n",
    "              \n",
    "        gradient = tape.gradient(loss, self.policy.trainable_variables)\n",
    "        \n",
    "        self.policy.optimizer.apply_gradients(zip(gradient, self.policy.trainable_variables))\n",
    "        \n",
    "        self.state_memory = []\n",
    "        self.action_memory = []\n",
    "        self.reward_memory = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  1 score 189.0 avg score 189.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    agent = Agent(alpha=0.0005, gamma=0.99, n_actions=7)\n",
    "\n",
    "    env = gym.make('AssaultNoFrameskip-v0')\n",
    "    env = gym.wrappers.AtariPreprocessing(env, noop_max=30, screen_size=84, terminal_on_life_loss=False, grayscale_obs=True, grayscale_newaxis=True, scale_obs=True)\n",
    "    \n",
    "    score_history = []\n",
    "\n",
    "    i = 0\n",
    "    \n",
    "    while True:\n",
    "        done = False\n",
    "        score = 0\n",
    "        observation = env.reset()\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            \n",
    "            #action = env.action_space.sample()\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            agent.store_transition(observation, action, reward)\n",
    "            observation = observation_\n",
    "            score += reward\n",
    "        \n",
    "        score_history.append(score)\n",
    "        agent.train()\n",
    "\n",
    "        i+= 1\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "        print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score)\n",
    "        if avg_score > 200:\n",
    "            print(\"Solved at episode {}!\".format(i))\n",
    "            break\n",
    "\n",
    "    filename = 'Assault.png'\n",
    "    plotLearning(score_history, filename=filename, window=100)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8f707cb8d373457ec1c7f3b1ea9e99b826cfdf12e940ff2e7b2bf43f4a978dd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
