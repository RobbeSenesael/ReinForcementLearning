{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "RANDOM_SEED = 5\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('AssaultNoFrameskip-v0')\n",
    "env = gym.wrappers.AtariPreprocessing(env, noop_max=30, screen_size=84, terminal_on_life_loss=False, grayscale_obs=True, grayscale_newaxis=True, scale_obs=True)\n",
    "\n",
    "print(\"Action Space: {}\".format(env.action_space))\n",
    "print(\"State space: {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "epochs, rewards = 0, 0\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    #sleep(0.1)\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    rewards  += reward\n",
    "    epochs += 1\n",
    "env.close()\n",
    "print(f\"Number of steps: {epochs}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Steps = []\n",
    "for episode in range(100):\n",
    "        n_steps_episode = 0\n",
    "        total_training_rewards = 0\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            total_training_rewards += reward  \n",
    "            observation = new_observation\n",
    "            \n",
    "            n_steps_episode += 1.\n",
    "\n",
    "        Steps.append(n_steps_episode)\n",
    "\n",
    "np.mean(Steps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state_shape, action_shape):\n",
    "  learning_rate = 0.001\n",
    "  init = tf.keras.initializers.HeUniform()\n",
    "  model = keras.Sequential() \n",
    "  model.add(keras.layers.Input(shape=(state_shape)))\n",
    "  model.add(keras.layers.Conv2D(32,kernel_size=(3,3),strides=(2,2)))\n",
    "  model.add(keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "  model.add(keras.layers.BatchNormalization())\n",
    "  model.add(keras.layers.Flatten())\n",
    "\n",
    "  model.add(keras.layers.Dense(128, activation='relu', kernel_initializer=init))\n",
    "  model.add(keras.layers.Dense(128, activation='relu', kernel_initializer=init))\n",
    "  model.add(keras.layers.Dense(action_shape, activation='linear', kernel_initializer=init))\n",
    "  model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes = 250\n",
    "test_episodes = 100\n",
    "\n",
    "epsilon = 1 \n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01\n",
    "decay = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main prediction Model (updated every 4 steps)\n",
    "model = agent(env.observation_space.shape, env.action_space.n)\n",
    "# Target Model (updated every 100 steps)\n",
    "target_model = agent(env.observation_space.shape, env.action_space.n)\n",
    "\n",
    "# Set the weights of the target model to be equal to the weights of the prediction model\n",
    "target_model.set_weights(model.get_weights())\n",
    "\n",
    "# Initialize the replay_memory\n",
    "replay_memory = deque(maxlen=50_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, replay_memory, model, target_model, done):\n",
    "    # We define the learning rate and discount factor\n",
    "    learning_rate = 0.7 # Learning rate\n",
    "    discount_factor = 0.618\n",
    "\n",
    "    # We will only start training the model, once we have a replay memory of at least 1000 steps.\n",
    "    MIN_REPLAY_SIZE = 1000\n",
    "    # Therefore, if we have less than 1000, we will skip the training (by using return in the function)\n",
    "    if len(replay_memory) < MIN_REPLAY_SIZE:\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    # We will train our model on 500 random steps from the replay memory each time\n",
    "    batch_size = 500\n",
    "    mini_batch = random.sample(replay_memory, batch_size)\n",
    "    \n",
    "    ## We need to define our predictor (X) and outcome (Y) to be able to train the model.  \n",
    "        # Note that this environment uses 'observation' to refer to the 'state'\n",
    "        # It is important to realize that in the replay_memory, we will store the different steps as \n",
    "          # [observation, action, reward, new_observation, done]\n",
    "    \n",
    "    # First, we transform or selected batch into readable states for the model (so, we select the first element [0] from the replay memory: the 'observation')\n",
    "    current_states = np.array([transition[0] for transition in mini_batch])\n",
    "        # Then, we predict the current Q-values, using the prediction network \n",
    "    current_qs_list = model.predict(current_states)\n",
    "        # Then we extract the next state that the agent will end up in , so the fourth elemenent [3]\n",
    "    new_current_states = np.array([transition[3] for transition in mini_batch])\n",
    "        # We predict the q-value of the next state by using the target model\n",
    "    future_qs_list = target_model.predict(new_current_states)\n",
    "  \n",
    "  \n",
    "        # Define 2 empty vectors for the X and Y-values\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "        #  Calculate what the maximum q-value is for the next step (to be able to update the q-value), as long as the episode is not done\n",
    "    for index, (observation, action, reward, new_observation, done) in enumerate(mini_batch):\n",
    "        if not done:\n",
    "            max_future_q = reward + discount_factor * np.max(future_qs_list[index])\n",
    "        else:\n",
    "            max_future_q = reward\n",
    "            \n",
    "        # We update the q-value, using the bellman-equation\n",
    "        current_qs = current_qs_list[index]\n",
    "        current_qs[action] = (1 - learning_rate) * current_qs[action] + learning_rate * max_future_q\n",
    "        \n",
    "        # Store the current state in the X-vector, to be used as predictor\n",
    "        X.append(observation)\n",
    "        # Store the updated q-value in the Y-vector to be used as the outcome\n",
    "        Y.append(current_qs)\n",
    "    \n",
    "    ## Finally, train the model\n",
    "    model.fit(np.array(X), np.array(Y), batch_size=batch_size, verbose=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "Steps = []\n",
    "\n",
    "# We will need to count how many steps has passed to be ably to update the main and target model \n",
    "steps_to_update_target_model = 0\n",
    "\n",
    "# We will train our agent for a set number of episodes\n",
    "for episode in range(train_episodes):\n",
    "    # Set the number of steps (and rewards) for the episode at 0 to start (we will add 1 after each step)\n",
    "    n_steps_episode = 0\n",
    "    total_training_rewards = 0\n",
    "    # Set the state ('observation') at the beginning of a random episode\n",
    "    observation = env.reset()\n",
    "    # We start with a game that is not done\n",
    "    done = False\n",
    "    # As long as the episode is not done, we will keep playing the game\n",
    "    while not done:\n",
    "        # for each step, add 1 to the counter keeping track when to update the models\n",
    "        steps_to_update_target_model += 1\n",
    "        # Because of this code, you can see the game in a seperate frame\n",
    "       # if True:\n",
    "       #     env.render()\n",
    "        \n",
    "        # Choose a random number to act out the epsilon greedy stragety\n",
    "        random_number = np.random.rand()\n",
    "        if random_number <= epsilon:\n",
    "            # Explore (use random action) when the number is smaller than epsilon\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        else:\n",
    "            # Or exploit (choose the best known action) when it is larger\n",
    "            \n",
    "                # Transform the state into the right format for the deep learning model\n",
    "            observation_reshaped = np.array([observation],order = 'C')\n",
    "            observation_reshaped.resize(1,84,84,1)\n",
    "            observation_reshaped.shape\n",
    "            #observation_reshaped = observation.reshape([1, observation.shape[0]])\n",
    "                # predict the q-values of all possible actions using the main model\n",
    "            print(\"Model predicted: \")\n",
    "            print(model.predict(observation_reshaped))\n",
    "            predicted = model.predict(observation_reshaped).flatten()\n",
    "                # choose the action based on the predicted q-values\n",
    "            action = np.argmax(predicted)\n",
    "                # Store the results of this action\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        total_training_rewards += reward\n",
    "                # Add the step to the replay memory (from which will be sampled to train the model)\n",
    "        replay_memory.append([observation, action, reward, new_observation, done])\n",
    "\n",
    "        # Update the Main Network using the Bellman Equation -> here we call the trainingsfunction we defined above\n",
    "            # We only update our model every 4 steps\n",
    "        if steps_to_update_target_model % 4 == 0 or done:\n",
    "            train(env, replay_memory, model, target_model, done)\n",
    "        \n",
    "        # Set the new state to the currect state to be able to perform the next step\n",
    "        observation = new_observation\n",
    "        \n",
    "        # Count the step you just took\n",
    "        n_steps_episode += 1\n",
    "        \n",
    "        # At the end of the episode: print the number of steps for that episode and store it in the Steps-vector\n",
    "        if done:\n",
    "            print('{} Total training rewards: {} after n steps = {}'.format(episode, total_training_rewards, n_steps_episode))\n",
    "            Steps.append(n_steps_episode)\n",
    "\n",
    "        # If 100 steps have passed, copy the network weights of the main model to the target model\n",
    "            if steps_to_update_target_model >= 100:\n",
    "                target_model.set_weights(model.get_weights())\n",
    "                steps_to_update_target_model = 0\n",
    "            break\n",
    "        # Adjust epsilon after an episonde is done\n",
    "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay * episode)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_res(values, title=''):   \n",
    "    ''' Plot the reward curve and histogram of results over time.'''\n",
    "   \n",
    "    # Define the figure\n",
    "    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n",
    "    f.suptitle(title)\n",
    "    ax[0].plot(values, label='score per run')\n",
    "    ax[0].axhline(800, c='red',ls='--', label='goal')\n",
    "    ax[0].set_xlabel('Episodes')\n",
    "    ax[0].set_ylabel('Reward')\n",
    "    ax[1].tick_params(axis='x', colors='white')\n",
    "    ax[1].tick_params(axis='y', colors='white')\n",
    "    x = range(len(values))\n",
    "    ax[0].legend()\n",
    "    # Calculate the trend\n",
    "    try:\n",
    "        z = np.polyfit(x, values, 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax[0].plot(x,p(x),\"--\", label='trend')\n",
    "    except:\n",
    "        print('')\n",
    "    \n",
    "    # Plot the histogram of results\n",
    "    ax[1].hist(values[-50:])\n",
    "    ax[1].axvline(200, c='red', label='goal')\n",
    "    ax[1].set_xlabel('Scores per Last 50 Episodes')\n",
    "    ax[1].set_ylabel('Frequency')\n",
    "    ax[1].tick_params(axis='x', colors='white')\n",
    "    ax[1].tick_params(axis='y', colors='white')\n",
    "    ax[1].legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_res(Steps,'Random actions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import joblib\n",
    "#model.save('assault_deepQ.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs, rewards = 0, 0\n",
    "observation = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    #observation_reshaped = observation.reshape([1, observation.shape[0]])\n",
    "    observation_reshaped = np.array([observation],order = 'C')\n",
    "    observation_reshaped.resize(1,84,84,1)\n",
    "    observation_reshaped.shape\n",
    "    predicted = model.predict(observation_reshaped).flatten()\n",
    "    action = np.argmax(predicted)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    rewards  += reward\n",
    "    epochs += 1\n",
    "env.close()\n",
    "print(f\"Reward: {rewards}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Steps = []\n",
    "for episode in range(test_episodes):\n",
    "        n_steps_episode = 0\n",
    "        total_training_rewards = 0\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            #observation_reshaped = observation.reshape([1, observation.shape[0]])\n",
    "            observation_reshaped = np.array([observation],order = 'C')\n",
    "            observation_reshaped.resize(1,84,84,1)\n",
    "            observation_reshaped.shape\n",
    "            predicted = model.predict(observation_reshaped).flatten()\n",
    "            action = np.argmax(predicted)\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            total_training_rewards += reward  \n",
    "            observation = new_observation\n",
    "            \n",
    "            n_steps_episode += 1\n",
    "        print('{} Total steps = {}'.format(episode,n_steps_episode))\n",
    "        Steps.append(n_steps_episode)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8f707cb8d373457ec1c7f3b1ea9e99b826cfdf12e940ff2e7b2bf43f4a978dd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
